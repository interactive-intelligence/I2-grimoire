\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Machine Learning}{6}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Data Splitting: Train and Test Sets}{6}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regression and Classification Tasks}{7}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Regression: Regression}{7}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Fundamentals of Linear Regression}{7}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Understanding Sum of Squared Errors (SSE)}{7}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An illustration of two lines of fit that would produce a low and high SSE respectively. The line that fits the data better has a low SSE}}{8}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:seefits}{{1}{8}{An illustration of two lines of fit that would produce a low and high SSE respectively. The line that fits the data better has a low SSE}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Minimizing SSE: Simple Case}{8}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Minimizing SSE: General Case}{9}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Ridge and LASSO Regressions}{13}{subsection.2.5}\protected@file@percent }
\newlabel{fig:regression_comparison}{{\caption@xref {fig:regression_comparison}{ on input line 167}}{13}{Ridge and LASSO Regressions}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Showing how the L1 and L2 norms influence $\hat  {\textbf  {w}}$ through the nature of their manifestations on the loss landscape as a hyperdiamond and hypersphere respectively}}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:l1_l2}{{2}{15}{Showing how the L1 and L2 norms influence $\hat {\textbf {w}}$ through the nature of their manifestations on the loss landscape as a hyperdiamond and hypersphere respectively}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Logistic Regression: Classification}{16}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Fundamentals of Logistic Regression}{16}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Sigmoid Function and Its Role}{17}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A graph showing the sigmoid function.}}{17}{figure.caption.8}\protected@file@percent }
\newlabel{fig:sigmoid_function}{{3}{17}{A graph showing the sigmoid function}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Training with the Log-Likelihood Loss}{18}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Extensions to Multiclass Classification}{18}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Why It's Called “Regression”}{18}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}K-Means Clustering: Unsupervised Learning}{19}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Introduction to Unsupervised Learning}{19}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}How K-Means Clustering Works}{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Minimizing Within-Cluster Distance}{19}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Choosing the Number of Clusters}{20}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of a graph showing SSE vs. $k$, and the elbow that can be used to pick the optimal $k$.}}{20}{figure.caption.9}\protected@file@percent }
\newlabel{fig:SSEkElbowmethod}{{4}{20}{An example of a graph showing SSE vs. $k$, and the elbow that can be used to pick the optimal $k$}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Limitations of K-Means Clustering}{21}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}K-Means vs. K-Medians Clustering}{21}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Introduction to K-Medians Clustering}{21}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}K-Medians Use Cases}{22}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Key Differences Between K-Means and K-Medians}{22}{subsection.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  An image comparing and contrasted the clusters created from k-means and k-medians methods.}}{23}{figure.caption.10}\protected@file@percent }
\newlabel{fig:K-clustering}{{5}{23}{An image comparing and contrasted the clusters created from k-means and k-medians methods}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Other ML Concepts}{23}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Bias-Variance Tradeoff}{23}{subsection.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An illustration visualizing different balances between bias and variance for model fitting.}}{25}{figure.caption.11}\protected@file@percent }
\newlabel{fig:biasvariance}{{6}{25}{An illustration visualizing different balances between bias and variance for model fitting}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Convexity}{25}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An illustration visualizing a convex function and a non-convex function. Also shown is the problem that comes with optimizing non-convex functions: getting stuck at a local minima.}}{25}{figure.caption.12}\protected@file@percent }
\newlabel{fig:convexity}{{7}{25}{An illustration visualizing a convex function and a non-convex function. Also shown is the problem that comes with optimizing non-convex functions: getting stuck at a local minima}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}When to Use Machine Learning Algorithms}{26}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion (ML)}{26}{section.8}\protected@file@percent }
\@setckpt{ml}{
\setcounter{page}{27}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{float@type}{16}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{20}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{30}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{5}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{lstlisting}{0}
}
