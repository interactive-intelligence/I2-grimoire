\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Neural Networks}{6}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Fundamental Structure}{6}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visualization of neural networks}}{6}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neuralnetwork}{{1}{6}{A visualization of neural networks}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of how images are converted into vectors}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:img-vector}{{2}{7}{Visualization of how images are converted into vectors}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Flow of Information}{8}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An illustration of weights and biases connecting the first layer of our neural network to the first neuron of the second (hidden) layer.}}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:connectivity_basic}{{3}{9}{An illustration of weights and biases connecting the first layer of our neural network to the first neuron of the second (hidden) layer}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The Perceptron and XOR}{10}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of the XOR function, and how you cannot draw a single line that separates the classes (white and black dots).}}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:xor}{{4}{11}{Illustration of the XOR function, and how you cannot draw a single line that separates the classes (white and black dots)}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Non-Linearity and Activation Functions}{13}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introducing Nonlinearities}{13}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Common Activation Functions}{15}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plots of various activation functions}}{15}{figure.caption.9}\protected@file@percent }
\newlabel{fig:activationfuncs}{{5}{15}{Plots of various activation functions}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Backpropagation}{16}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Loss Functions}{16}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Derivatives and Gradients}{17}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of the function $f(x, y) = x^2 + y^2$}}{18}{figure.caption.10}\protected@file@percent }
\newlabel{fig:x2y2}{{6}{18}{Illustration of the function $f(x, y) = x^2 + y^2$}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustration of the function $f(x, y) = x^2 + y^2$, with a vector showing the direction of steepest ascent from point $(1,1)$}}{19}{figure.caption.11}\protected@file@percent }
\newlabel{fig:x2y2_2}{{7}{19}{Illustration of the function $f(x, y) = x^2 + y^2$, with a vector showing the direction of steepest ascent from point $(1,1)$}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Gradient Flow}{20}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Optimizers and Learning Rates}{22}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A demonstration of how different learning rates affect convergence of a loss function (in this graphic represented as $J(\theta ).$}}{22}{figure.caption.12}\protected@file@percent }
\newlabel{fig:lr}{{8}{22}{A demonstration of how different learning rates affect convergence of a loss function (in this graphic represented as $J(\theta ).$}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Regularization}{23}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Dropout}{23}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visualizing dropout in a fully connected neural network}}{24}{figure.caption.13}\protected@file@percent }
\newlabel{fig:dropout}{{9}{24}{Visualizing dropout in a fully connected neural network}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Batch Normalization}{24}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}When to Use Deep Learning/Neural Networks}{26}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion (DL)}{26}{section.6}\protected@file@percent }
\@setckpt{dl}{
\setcounter{page}{27}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{0}
\setcounter{float@type}{16}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{15}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{17}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{4}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{lstlisting}{0}
}
