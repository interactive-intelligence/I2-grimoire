\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}What is Reinforcement Learning?}{6}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem Definition}{6}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Common Symbols and Definitions}{6}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Markov Decision Process}{8}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}On vs. Off-Policy RL}{9}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Imitation Learning}{10}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Basic Behavior Cloning}{10}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Problems with Behavior Cloning}{12}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An illustration demonstrating the issues that arise from mode-averaging within behavior cloning algorithms. The green and blue peaks represent choosing to go left or right around the obstacle, and the black dashed line represents what a simple policy will converge on.}}{13}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:modeavg}{{1}{13}{An illustration demonstrating the issues that arise from mode-averaging within behavior cloning algorithms. The green and blue peaks represent choosing to go left or right around the obstacle, and the black dashed line represents what a simple policy will converge on}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Proofs for Problems with Behavior Cloning}{13}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Mode Averaging}{13}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The blue curve is our single-mode Gaussian decision policy and the bimodal orange line is our expert policy (can go left or right around the tree). The top graph shows what we would like to happen, which is choosing a mode and sticking with it. However, the bottom graph shows what actually happens when we minimize forward KL divergence. This is mode-averaging behavior (crashing into the tree).}}{16}{figure.caption.6}\protected@file@percent }
\newlabel{fig:diffkl}{{2}{16}{The blue curve is our single-mode Gaussian decision policy and the bimodal orange line is our expert policy (can go left or right around the tree). The top graph shows what we would like to happen, which is choosing a mode and sticking with it. However, the bottom graph shows what actually happens when we minimize forward KL divergence. This is mode-averaging behavior (crashing into the tree)}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}DAgger Algorithm}{16}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A visualization of how DAgger helps keep the agent performing well. We first see the agent ($\hat  {x}$) deviate from the expert ($x$), but we then take these observations of where the agent failed and give it instructions on how to get back on track. With enough iterations, a lot of failure cases can be mitigated.}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:daggerviz}{{3}{17}{A visualization of how DAgger helps keep the agent performing well. We first see the agent ($\hat {x}$) deviate from the expert ($x$), but we then take these observations of where the agent failed and give it instructions on how to get back on track. With enough iterations, a lot of failure cases can be mitigated}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Policy Gradient}{18}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Basic Policy Gradient}{18}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An illustration showing how the agent and environment communicate through actions, states, and rewards in a traditional RL setting. The subscript $t$ is for timestep.}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:rlsystem}{{4}{18}{An illustration showing how the agent and environment communicate through actions, states, and rewards in a traditional RL setting. The subscript $t$ is for timestep}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Advanced Policy Gradient Concepts}{21}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Return-to-Go}{21}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Baseline Function}{21}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Natural Policy Gradient}{22}{subsection.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces There are a few items in this diagram. The parabola and tangent show the issues of large gradient steps when you have only a monte-carlo approximation to the true gradient. You can wildly overshoot where you should be ($\theta _{i+1}$).The dimensions of the stretched oval represent the parameters of the policy, and how steps in the wrong direction can be disastrous since the policy is very sensitive to small parameter shifts. What we don't have in RL is an easy objective to optimize on like the circle. These problems together are why NPG is useful.}}{23}{figure.caption.9}\protected@file@percent }
\newlabel{fig:npg}{{5}{23}{There are a few items in this diagram. The parabola and tangent show the issues of large gradient steps when you have only a monte-carlo approximation to the true gradient. You can wildly overshoot where you should be ($\theta _{i+1}$).The dimensions of the stretched oval represent the parameters of the policy, and how steps in the wrong direction can be disastrous since the policy is very sensitive to small parameter shifts. What we don't have in RL is an easy objective to optimize on like the circle. These problems together are why NPG is useful}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Proof for Gradient of the PG Objective}{23}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Deep Q-Learning}{26}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Q-Functions and Bellman Equations}{26}{subsection.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A visualization of what the Q-function takes into account. It averages the rewards from time $t$ and forward across considers many paths that policy $\pi _\theta $ could theoretically take. The output of a Q-function is called a Q-value. It signifies the ``quality'' of the given state-action pair.}}{27}{figure.caption.10}\protected@file@percent }
\newlabel{fig:qlearning}{{6}{27}{A visualization of what the Q-function takes into account. It averages the rewards from time $t$ and forward across considers many paths that policy $\pi _\theta $ could theoretically take. The output of a Q-function is called a Q-value. It signifies the ``quality'' of the given state-action pair}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Making Q-Learning Work}{29}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Replay Buffer and Memory}{29}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Optimization Stability and Polyak Averaging}{29}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Explore vs. Exploit and epsilon-Greedy Search}{30}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}When to Use Reinforcement Learning}{31}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion (RL)}{32}{section.10}\protected@file@percent }
\@setckpt{rl}{
\setcounter{page}{33}
\setcounter{equation}{0}
\setcounter{enumi}{7}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{10}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{float@type}{16}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{23}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{26}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{4}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{lstlisting}{0}
}
